机器学习

1. 人工智能, 机器学习, 深度学习

    人工智能
        机器学习
            经典机器学习 : 基于少量数据, 大量算法
            基于神经网络的机器学习 : 基于大量数据, 少量算法, 通过每个神经元加权运算得出规律
                浅层学习
                深层学习(深度学习)
            强化学习 : 奖惩机制似的学习
            迁移学习 : 可以从一个点迁移到另一个点的学习

2. 机器学习基本类型
    1) 有监督学习 : 根据已知的输入和输出, 建立联系他们的模型, 根据该模型对未知输出的输入进行判断
        (1) 回归, 以无限连续域的形式表示输出
        (2) 分类, 以有限离散域的形式表示输出
    2) 无监督学习 : 在一组没有已知输出(标签)的输入中, 根据数据的内部特征和联系, 找到某种规则, 进行族群的划分--聚类
    3) 半监督学习 : 从一个相对有限的已知结构中利用有监督学习的方法, 构建基本模型, 通过对未知输入和已知输入的比对, 判断其输出, 扩展原有的已知领域

3. 机器学习的基本过程
    数据采集 --> 数据清洗 --> 数据预处理 --> 选择模型 --> 训练模型 --> 测试模型 --> 使用模型
     原材料      去杂质          准备         算法        规则         检验       生产环境

---------------------------------------

4. 数据预处理
             一列一特征
    一行一样本  x x x x
               x x x x
               x x x x
    使用二维数组来表示一个数据集
    这样的数据样本和数据库能很好的匹配 

    1) 均值移除
        为了统一样本矩阵中不同特征的基准值和分散度, 可以将各个特征的平均值调整为0, 标准差调整为1, 这个过程称为均值移除
        保证了数据的个体不同.
        平均值调为0
            每个值减去均值
            a, b, c, 均值m
            m' = (a-m + b-m + c-m)/3
        标准差调为0
            每个值除以标准差
            A, B, C
            s = sqrt( (A^2 + B^2 + C^2)/3 )
            A/s, B/s, C/s
            s' = sqrt((A^2/s^2 + B^2/s^2 + C^2/s^2)/3)

        sklearn.preprocessing.scale(原始样本矩阵) -> 均值移除后的样本举证
        示例 01.均值移除 std.py

    2) 范围缩放
        统一样本矩阵中不同特征(针对列)的最大值和最小值范围, 让其范围标准相同
        线性相关性
            kx + b = y

        sklearn.preprocessing.MinMaxScaler(feature_range=期望最大值和最小值) -> 范围缩放器对象
        方位缩放器.fit_transform(原始样本矩阵) -> 范围缩放后的样本矩阵
        示例 02. 范围缩放mms.py
    
    3) 归一化
        为了用占比来表示特征, 用每个样本(行)的特征值(列)除以该样本的特征值的绝对值之和, 以使每个样本的特征值绝对值之和为1
        sklearn.preprocessing.normalize(原始样本矩阵, norm="l1") -> 归一化后的样本矩阵
            l1即L1范数, 矢量中各个元素绝对值之和
            l2, 矢量中各元素平方值之和

    4) 二值化
        用0和1来表示样本矩阵中相对于某个给定阀值高于或者低于它的元素
        sklearn.preprocessing.Binarizer(threshold=阀值) -> 二值化器
        二值化器.transform(原始样本矩阵) -> 二值化后的样本矩阵

    5) 独热编码
        用约定0, 1的数字来表示每列中的数字的位置或下标, 这样就与原数据形成了一套可查字典, 变形后的数据可以根据此来查询到原值
        sklearn.preprocessing.OneHotEncoder(sparse=是否使用压缩格式, dtype=元素类型) -> 独热编码器
        独热编码器.fit_transform(原始样本矩阵) -> 独热编码后的样本矩阵, 使用自己已有的编码表字典
        示例 独热编码ohe.py

    6) 标签编码
        将字符形式的特征值(列)映射为整数
        sklearn.preprocessing.LabelEncoder() -> 标签编码器
        标签编码器.fit_transform(原始样本矩阵) -> 编码样本矩阵, 构建编码字典
        标签编码器.transform(原始样本矩阵) -> 编码样本矩阵, 使用编码字典
        标签编码器.inverse_transform(编码样本矩阵) -> 原始样本矩阵, 
        示例 标签编码lab.py

5. 线性回归
    m个输入样本 -> m个输出样本
    kx + b -> y

    1) 预测函数: 联系输出和输入的数学函数
        其中的k和b称为模型参数, 根据已知输入样本和对应的输出标签来训练得出

    2) 均方误差: 用每个已知输入样本所对应的实际输出标签和由模型预测出来的输出标签之间的误差平方的平均值 
        实际值y, 预测值y', 遵循关系 kx + b = y
        ( (y1-y1')^2 + (y2-y2')^2 + ... + (ym-ym')^2 )/m

    3) 成本函数: 将均方误差看做是关于模型参数的函数, 谓之成本函数, 记做J(k,b)
    线性回归问题的本质就是寻找能够使成本函数J(k,b)取到极小值的模型参数

    4) 梯度下降
        求极值 loss = J(k,b) , 这是一个三维算式, 求其满足loss最小的k, b值

    5) 接口
        sklearn.linear_model.LinearRegression() -> 线性回归器
        线性回归器.fit(输入样本, 输出标签)
        线性回归器.predict(输入样本) -> 预测输出标签
        import sklearn.metrics
    
    6) 复用
        通过pickle将内存中的模型对象写入磁盘文件, 或从磁盘文件中载入内存, 以此保存训练好的模型, 以备复用
        示例: 梯度下降line, save, load .py

    6) 岭回归
        注意不符合的数据, 往往这样的数据是错误的, 这样的数据会对分析起到干扰作用
        loss = J(k,b) + S(样本权重) × 正则强度(惩罚系数)
        sklearn.linear_model.Ridge(正则强度, fit_intercept=是否修正截距b, max_iter=最大迭代次数) -> 岭回归器
        示例 岭回归rdg.py

7. 欠拟合与过拟合
    欠拟合: 无论是训练数据还是测试数据, 模型给出的预测值和真实值都存在较大的误差.
    过拟合: 模型对于训练数据具有较高的精度, 但对测试数据则表现极差. 模型过于特殊, 不够一般(泛化).
    岭回归就是将特殊数据忽视, 而控制过拟合.
    模型越复杂, 越容易出现过拟合; 模型越简单, 越容易出现欠拟合.

8. 多项式回归
    增加数据复杂度
    x -> y           y = kx + b 
    x x^2 -> y       y = k1x^2 + k2x + b
    x x^2x^3 -> y    y = k1x^3 + k2x^2 + k3x + b
    sklearn.preprocessing.PolynomialFeatures(最高次数) -> 多项式特征拓展器
    sklearn.pipline.make_pipe(多项式特征拓展器, 线性回归器) -> 多项式回归器
        管线用于将两个线性式连接起来, 将上一项的输出作为下一项的输入
    示例: 多项式回归poly.py

9. 决策树
    即可以用于回归, 也可以用于分类
        回归使用平均数, 分类使用投票
    相似的输入, 会有相似的输出
    优化:
        1) 结合也为优先选择优先的主要特征, 划分子表, 降低决策树的高度.
        2) 根据香农定理计根据每一个特征划分子表前后的信息熵差, 选择熵减少量最大的特征, 优先参与子表划分.
        3) 集合算法: 多种树根据不同方法, 构建多个决策树, 利用他们的预测结果, 按照取均(回归)或投票(分类)的方法, 来产生最终的预测结果.
    建立多棵树
        自助聚合: 采用有放回的抽样规则, 从m个样本中随机抽取n个样本, 构建一棵决策树, 重复以上过程b次, 得到b棵决策树, 
                 利用每棵决策树的预测结果, 根据平均或者投票得到最终预测结果.
        随机森林: 自助算法的基础上更进一步, 对特征也应用自助聚合, 即每次训练时, 不使用所有的特征构建树结构, 
                 而是随机选择部分特征参与构建, 以此避免特殊特征对预测结果的影响.
        正向激励: 初始化时, 针对m个样本分配初始权重(也可自定义), 然后根据这个带有权重的模型预测训练样本, 针对那些预测错误的样本, 提高其权重, 
                 再构建一棵决策树模型, 重复以上过程, 得到b棵决策树, 最后按照取均或投票的方法, 产生最终预测结果.

    sklearn.tree.DecisionTreeRegressor() -> 决策树回归器
    sklearn.ensemble.AdaBoostRegressor(元回归器, n_estimators=评估器数, random_state=随机种子源) -> 正向激励回归器
        n_estimators, 有多少棵树
    sklearn.ensemble.RandomForestRegressor(max_depth=最大树高, n_estimators=评估器数, min_samples_split=划分子表的最小样本数) -> 随机森林回归器

    示例: 决策树house.py
    
    决策树模型.feature_importances_ : 特征的重要性排序

    示例: 决策树模型fi.py
    示例: 决策树模型bike.py
